# Measuring the Initial Congestion Window of Popular Webservers

In this repository, we attempt to reproduce measurements of the initial congestion window (ICW) size used by popular web servers. We aim to replicate the size measurements presented in the 2001 paper "On Inferring TCP Behavior" by Jitendra Padhye and Sally Floyd on the modern internet.

## Installation and Reproduction Steps

The results are designed to be reproduced on a machine running Ubuntu 14.04. Other platforms (i.e. Windows, macOS) may run the tester, but this requires ensuring that the operating system or the hypervisor (if running on a VM) does not change or interrupt the packet communication generated by our tester. (This may require configuration change on local firewall rules or hypervisor settings. The default configuration uses `iptables` to reproduce the results on a non-VM Ubuntu machine.

1. Get a copy of the code

    ```
    git clone https://github.com/serhatarslan-hub/cs244-ICW_testing.git
    ```

2. Install the python dependencies and make sure they're accessible to the root user:
 
    ```
    cd cs244-ICW_testing
    sudo pip install -r requirements.txt
    ```

3. Reproduce the results with a modern list of URLS (this will take some time):

    ```
    sudo python run_icw_test.py --mss 64 --url_list urls/QuantcastPopularURLs.txt
    ```  
    Please note that we are using the list of most popular 20 thousand URLS provided by [Quantcast](https://www.quantcast.com/top-sites/).

To estimate the initial congestion window of an IP or URL of your choice `YOUR_IP` and a specific page of your choice `PATH_TO_PAGE`, simply run:

```
sudo python run_icw_test.py --host YOUR_IP --rqst_page PATH_TO_PAGE
```

To perform the tests in a more realistic environment, set the `--mss` value to 1460 and specify a large object on the webserver in `--rqst_page` to fill multiple packets of this size.
  
Our tests request for a non-existing very long page from every URL. This is implemented to make sure response content is long enough to fill the ICW as suggested by [[Rüth et al. '17]](https://conferences.sigcomm.org/imc/2017/papers/imc17-final43.pdf). To request for the main page of the URL, simply pass `--main_page` argument while running the tester, e.g.

```
sudo python run_icw_test.py --mss 100 --url_list urls/QuantasPopularURLs.txt --main_page
```  

For more options, see:

```
python run_icw_test.py --help
```

## Brief Description  

Padhye & Floyd published their original ICW test in 2001 with in an attempt to survey the general behavior of TCP implementations of the internet at the time. Historically, TCP implementations differed widely, so it remains as important today as it was in 2001 to get a picture on the diversity of parameters used on the internet to ensure fairness and stability. Understanding the big picture of the internet can then help in determining standards, simulations, and new applications.

Padhye & Floyd's initial ICW was presented with the release of the TCP Behavior Inference Tool (TBIT), which performs six different tests on publicly available web servers to understand their TCP behavior. The tests covered the ICW value, the choice of congestion control algorithm (CCA), conformant congestion control (CCC), implementation of selective acknowledgements (SACK), TCP timeout duration, and responses to ECN. In this report, we only attempt to reproduce the ICW results of popular web servers.

The congestion window size is one of two metrics that determine how much data can be sent before any acknowledgement of arrival is received. Popular congestion control algorithms start from a relatively small value of congestion window size and slowly increase until congestion is perceived. The ICW determines how much data to be sent without any feedback on current congestion on the network. As a consequence, the choice of ICW is a trade-off between under-utilizing the available capacity and putting too much stress on the network. 

In 2002, RFC 3390 "Increasing TCP's Initial Window" set a standard for determining ICW for implementors of TCP congestion control. The ICW was to be set as

```
min (4*MSS, max (2*MSS, 4380 bytes))
```

where `MSS` is the maximum segment size (in bytes) which set by the negotiation of the two end-hosts during the TCP handshake. (Both endpoints would send their choice of the `MSS` and the smaller one was to be choosen.) We discuss our evaluation of this function and its effect on the results of our ICW tests [below](#discussion).

## The ICW Test

The ICW test proposed by Padhye & Floyd proceeds as follows:

- The tester performs a simple TCP handshake with the end host and specifies a small `MSS` value.
- The tester sends a packet with an HTTP GET request.
- Following the request, the tester sends no additional acknowledgements for arriving packets. This results in a timeout on the web server after `ICW * MSS` amount of data has been sent.
- Once a timeout is detected by the server but after filling the initial congestion window, it will begin retransmits the previosly sent packets.
- The tester then interprets this retransmission as the conclusion of the ICW and infers the ICW size as the total number of bytes received up to this point divided by the `MSS`.

In order to infer the ICW size, we need to ensure that the web server will send at least `ICW * MSS` amount of data. Finding a large file on every web server is difficult so Padhye & Floyd attempt to solve this problem by keeping the `MSS` small (the paper uses a value of 100 bytes). This is problematic because the most common MSS size on today's internet is 1460 bytes. We discuss this consideration below.

## Reproduction Philosophy

Our code aims to reproduce only the initial congestion window size measurements of Padhye & Floyd by reproducing Tables 2 and 3 in the original paper. To make our results comparable, we aimed to base our reproduction as closely as possible on the written description in Padhye & Floyd. However, some small modifications were necessary to make this reproduction possible. We then address issues with this reproduction on the modern internet, specifically the issue of an artificially small `MSS` and describe experiments for reproducing the test with a larger `MSS` on the modern internet.

We did not use TBIT (the tool that authors used for their tests) during our reproduction due to compatibility issues. TBIT was implemented for the BSD operating system 19 years ago (source code available at www.aciri.org/tbit/). Although a patch for Linux competibility was published in 2004, there remain several compatibility issues with modern Linux distributions. We implemented our own initial congestion window size tester in Python 2.7 using the Scapy packet manipulation module. We chose Scapy for its simplicity and flexibility for packet by packet content manipulation. We discuss complications with this choice below.

During our preliminary tests, we realized that relatively large group of the web servers have adopted an ICW size of >> 5 `MSS` sized packets. As a result, we extended the table 3 presented on the paper and with commonly encountered ICW configurations.

Although we did not directly use TBIT, the testing methodology of our implementation follows the descriptions given in the Padhye & Floyd paper step by step. The categorization of web servers follows that of the paper exactly, and error cases are implemented as desccribed in the paper with small  exceptions described below.

### A practically large `MSS`?

While Padhye & Floyd's TBIT tool errored out whenever the server responded with a `MSS` bigger than advertised, we found this impractical for an `MSS` of 100 bytes as layed out in the paper. Many modern web servers will ignore requests for such a small `MSS` and when our tool is run in a commercial cloud environment such as Google Cloud, we found that networking infrastructure as well as virtualization platforms along the way often enforce a larger `MSS`. We thus don't penalize a server for returning data packets greater than our requested size and simply compute ICW as `total_payload_size / MSS`.  

We measure ICW consistently with the unit definition of `cwnd`, in bytes. As long as the ICW is filled up, the total amount of payload sent will be equal to `cwnd`. If packet loss occurs along the way, the received payload size may be smaller than what is sent, but our implementation follows the sequence numbers of the received packets and terminates the test when a loss is detected. We also detect whether the response has finished before filling up the `cwnd` by catching any `FIN`, `RST` packets and/or retransmissions. In order to make sure `FIN` packets are sent at the end of responses, we use the `Connection: Close` attribute in our `GET` requests.

However, as we alluded to, it is reasonable to ask whether running such a test with an `MSS` of 64 or 100 can reasonably return meaningful results on the modern internet, when common `MSS` values are far from this. To address this question, we perform our test on two websites with known large objects (`stanford.edu` and `youtube.com`) and report `ICW` results for different `MSS` sizes.

### Finding large objects  

Padhye & Floyd request the main pages of the URLs during their ICW tests. Then they state the risk of not maxing out the ICW with the content of the main page. As a solution to this risk, we follow a method similar to that of Rüth et al. 2017 for ensuring that the response to our GET request maxes out `MSS*ICW` bytes. We simply make up a fairly large request URL, i.e.

```
www.stanford.edu/AAAAAaaaaaBBBBBbbbbb...
```

This almost always ensures either a `301 Moved Permanently` or a `404 Not Found` error. In both cases, the response typically contains the initial URL (and more), pushing us past the required window.  

Although the large URL trick doesn't guarantee a large response, during our preliminary tests we realized that most of the websites had relatively small main page content. Our tool can be re-run to request the content of specific large objects.

## Results

Below we present our results for the Padhye & Floyd reproduction and our experiments with a larger `MSS`.

### Padhye & Floyd Reproduction

[ to do ]

### Larger `MSS`

[ to do ]

## Discussion

[ to do ]

## Complications and Limitations

- **Use of Scapy.** In using Scapy for our replication, we found that the tool can be slow at setting up sniffers, especially when running in virtualized environments. The default packet-receiving function `sniff()` requires us to use the slower `lfilter()` method for filtering packets in virtualized envirionments. With that setting, we consistently observed Scapy missing the first few packets before its sniffer was provisioned right after sending our GET request. We especially observed this case when we worked on a VM provisioned in an extremely high performance data center and queried URLs like www.google.com, likely only a few fiber hops away in the same datacenter (we tested this on Google Cloud). To circumvent this issue, we had to set up the sniffer asynchronously and ensure it was set up by the time we sent our first GET packet out. 

- **OS port blocking.** When using a tool like Scapy to send out raw L3 packets without sockets, the OS kernel closes any incoming connection with a RST packet before we even have a chance to process it. To avoid this, we had to set up a firewall rule before we start sending out any packets. We went with a blanket rule to avoid sending any RSTs on the source port:
 ```iptables -D OUTPUT -p tcp --sport %d --tcp-flags RST RST -j DROP```
where `%d` is our current evaluation port (unique for each URL and trial). After each test, we revert the firewall rule and close the connection by manually sending RST packet with Scapy. Please note that the provided firewall rules are for Linux operating system. Using other operating systems for running our implementation may still encounter the given port blocking problem.  

- **Default Hypervisor Configurations.** During our initial experiments, we realized that our hypervisor (Oracle VM Virtualbox) changed the `MSS` option in the outgoing packets to 1460 bytes even when we manually set it to different values. This was mainly because of the default behavior of the hypervisor itself as reported in [this bug report](https://www.virtualbox.org/ticket/15256). Since this issue prevents obtaining the desired behavior from the web servers, the following steps may be helpful to overcome the problem:  

    1. Use the bridged networking option for Virtualbox. (Go to Machine > Settings > Network > Adapter 1 and set it to "Bridged")  

    2. Set DHCP for the connected interface statically and steal the IP information of the host interface to connect the VM interface to the Internet. Namely, edit /etc/network/interfaces on the VM.  

	    ```
	    iface eth1 inet static  
	    	address [host ip]  
	            gateway [host gateway]  
	            broadcast [host broadcast]  
	            dns-nameservers 8.8.8.8
	    ```  

	    You may need to run `sudo ifdown eth1` and `sudo ifup eth1` after this or reboot the VM.

- **Batched Execution.** [ to do ]